{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd77528",
   "metadata": {},
   "source": [
    "\n",
    "# üìò Week 1 ‚Äì LangChain Core Foundations (General Track)\n",
    "\n",
    "> M·ª•c ti√™u tu·∫ßn n√†y: n·∫Øm v·ªØng **Prompt ‚Üí LLM ‚Üí Output Parser ‚Üí Chain**, l√†m ch·ªß **LCEL** (LangChain Expression Language) v√† **streaming**.  \n",
    "> Y√™u c·∫ßu: Python 3.10+, `pip install langchain langchain-openai tiktoken pydantic`\n",
    "\n",
    "**G·ª£i √Ω c·∫•u h√¨nh**:\n",
    "```bash\n",
    "pip install -U langchain langchain-openai tiktoken pydantic\n",
    "export OPENAI_API_KEY=sk-...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab3203",
   "metadata": {},
   "source": [
    "\n",
    "## Bu·ªïi 1 ‚Äì Hi·ªÉu c∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa LLM trong LangChain\n",
    "\n",
    "**B·∫°n s·∫Ω h·ªçc**: `ChatOpenAI`, c·∫•u tr√∫c messages, `invoke()` v√† `stream()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b785ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from dotenv import load_dotenv\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62820784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚öôÔ∏è Kh·ªüi t·∫°o LLM (OpenAI) ‚Äì d√πng GPT-4o l√†m v√≠ d·ª•\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "# S·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng OPENAI_API_KEY\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",\n",
    "    temperature=0.2\n",
    "    )\n",
    "\n",
    "resp = llm.invoke([\n",
    "    SystemMessage(content='B·∫°n l√† tr·ª£ gi·∫£ng python'),\n",
    "    HumanMessage(content=\"gi·∫£i th√≠ch str trong 1\")])\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "@tool\n",
    "def get_price(x: int) -> int:\n",
    "    \"\"\"Tr·∫£ v·ªÅ b√¨nh ph∆∞∆°ng c·ªßa s·ªë x.\"\"\"\n",
    "    return x * x\n",
    "model=ChatOpenAI(\n",
    "    model='gpt-5-mini',\n",
    "    temperature=0.2\n",
    ")\n",
    "agent=create_agent(\n",
    "    model=model,\n",
    "    tools=[get_price]\n",
    ")\n",
    "result=agent.invoke({    \n",
    "        'messages':[{\n",
    "            'role':'user',\n",
    "            'content':\"T√≠nh b√¨nh ph∆∞∆°ng c·ªßa 2 v√† 4\"\n",
    "        }\n",
    "\n",
    "    ]}\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419dfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üü¢ Streaming t·ª´ng token (hi·ªáu ·ª©ng g√µ ch·ªØ)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_stream = ChatOpenAI(model=\"gpt-5-mini\", temperature=0.3)\n",
    "stream = llm_stream.stream(\"Vi·∫øt 1 ƒëo·∫°n m√¥ t·∫£ 2 c√¢u v·ªÅ m√¥ h√¨nh LLM.\")\n",
    "for chunk in stream:\n",
    "    print(chunk.content or \"\", end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1788d5",
   "metadata": {},
   "source": [
    "\n",
    "### B√†i t·∫≠p nh·ªè\n",
    "1) Th·ª≠ ƒë·ªïi `model` v√† `temperature` ƒë·ªÉ quan s√°t kh√°c bi·ªát.  \n",
    "2) Vi·∫øt prompt y√™u c·∫ßu tr·∫£ v·ªÅ ƒë√∫ng **2 g·∫°ch ƒë·∫ßu d√≤ng**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57544a2",
   "metadata": {},
   "source": [
    "\n",
    "## Bu·ªïi 2 ‚Äì PromptTemplate & Chain c∆° b·∫£n (LCEL)\n",
    "\n",
    "**B·∫°n s·∫Ω h·ªçc**: `ChatPromptTemplate`, LCEL `|` ƒë·ªÉ gh√©p pipeline, t·∫°o **chain 1 b∆∞·ªõc** v√† **nhi·ªÅu b∆∞·ªõc**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b39746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"B·∫°n l√† tr·ª£ l√Ω n·ªôi dung, tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch.\"),\n",
    "    (\"human\", \"Vi·∫øt 1 slogan cho s·∫£n ph·∫©m: {product} v·ªõi tone {tone}.\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"product\": \"N∆∞·ªõc kho√°ng v·ªã yuzu\", \"tone\": \"tinh t·∫ø\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd, dumps\n",
    "from IPython.display import JSON\n",
    "\n",
    "# 1) L·∫•y dict ‚ÄúJSON-safe‚Äù\n",
    "obj = dumpd(chain)\n",
    "\n",
    "# 3) (tu·ª≥ ch·ªçn) In ra chu·ªói JSON c√≥ th·ª•t d√≤ng\n",
    "text = dumps(chain, pretty=True)\n",
    "print(text.encode('utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîó N·ªëi nhi·ªÅu b∆∞·ªõc b·∫±ng LCEL\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-mini\")\n",
    "\n",
    "# B∆∞·ªõc 1: sinh 5 slogan\n",
    "p1 = ChatPromptTemplate.from_template(\n",
    "    \"Sinh 5 slogan ng·∫Øn cho s·∫£n ph·∫©m {product}, tone {tone}. Ch·ªâ li·ªát k√™.\"\n",
    ")\n",
    "chain1 = p1 | llm | StrOutputParser()\n",
    "\n",
    "# B∆∞·ªõc 2: ch·ªçn 1 c√¢u hay nh·∫•t\n",
    "p2 = ChatPromptTemplate.from_template(\n",
    "\"\"\"Ch·ªçn 1 c√¢u hay nh·∫•t trong danh s√°ch sau v√† gi·∫£i th√≠ch ng·∫Øn l√Ω do:\n",
    "{candidates}\"\"\"\n",
    ")\n",
    "chain2 = p2 | llm | StrOutputParser()\n",
    "\n",
    "candidates = chain1.invoke({\"product\": \"fin68\", \"tone\": \"Chuy√™n nghi·ªáp\"})\n",
    "final = chain2.invoke({\"candidates\": candidates})\n",
    "print(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (1) C√ÅC SLOGAN ƒê·ªÄ C·ª¨ ===\n",
      "1. \"H√≤a tan v·ªã ngon, kh∆°i d·∫≠y nƒÉng l∆∞·ª£ng!\"\n",
      "2. \"C√† ph√™ 3in1 - Kh·ªüi ƒë·∫ßu ng√†y m·ªõi tr√†n ƒë·∫ßy h·ª©ng kh·ªüi!\"\n",
      "3. \"S·∫£ng kho√°i ngay l·∫≠p t·ª©c, th∆∞·ªüng th·ª©c m·ªçi l√∫c!\"\n",
      "4. \"NƒÉng ƒë·ªông t·ª´ng gi·ªçt, c√† ph√™ h√≤a tan tuy·ªát v·ªùi!\"\n",
      "5. \"Pha tr·ªôn ƒëam m√™, th∆∞·ªüng th·ª©c h·∫°nh ph√∫c!\"\n",
      "\n",
      "=== (2) SLOGAN T·ªêT NH·∫§T (JSON) ===\n",
      "{\n",
      "  \"best\": \"C√† ph√™ 3in1 - Kh·ªüi ƒë·∫ßu ng√†y m·ªõi tr√†n ƒë·∫ßy h·ª©ng kh·ªüi!\",\n",
      "  \"reason\": \"D·ªÖ nh·ªõ, th·ªÉ hi·ªán nƒÉng l∆∞·ª£ng t√≠ch c·ª±c cho ng√†y m·ªõi.\"\n",
      "}\n",
      "\n",
      "=== (3) CAPTION 1 C√ÇU ===\n",
      "Kh·ªüi ƒë·ªông ng√†y m·ªõi v·ªõi C√† ph√™ h√≤a tan 3in1 - NƒÉng l∆∞·ª£ng tr√†n ƒë·∫ßy ch·ªâ trong 1 ph√∫t! ‚òï‚ú®\n"
     ]
    }
   ],
   "source": [
    "# === Multi-stage Creative Chain (Fin68 style) ===\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 0) Models\n",
    "llm_gen = ChatOpenAI(model=\"gpt-4o-mini-mini\", temperature=0.7)   # sinh slogan (s√°ng t·∫°o)\n",
    "llm_pick = ChatOpenAI(model=\"gpt-4o-mini-mini\", temperature=0.0)  # ch·ªçn c√¢u (·ªïn ƒë·ªãnh)\n",
    "llm_cap  = ChatOpenAI(model=\"gpt-4o-mini-mini\", temperature=0.7)  # vi·∫øt caption\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (1) STAGE 1: Sinh 5 slogan\n",
    "# ------------------------------------------------\n",
    "p1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"B·∫°n l√† chuy√™n gia marketing c·ªßa Fin68. \"\n",
    "     \"Vi·∫øt ng·∫Øn g·ªçn, s√∫c t√≠ch, hi·ªán ƒë·∫°i v√† ph√π h·ª£p k√™nh digital.\"),\n",
    "    (\"human\",\n",
    "     \"H√£y vi·∫øt 5 slogan cho s·∫£n ph·∫©m {product} v·ªõi tone {tone}. \"\n",
    "     \"Ch·ªâ li·ªát k√™, m·ªói d√≤ng m·ªôt slogan, kh√¥ng gi·∫£i th√≠ch.\")\n",
    "])\n",
    "gen_chain = p1 | llm_gen | StrOutputParser()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (2) STAGE 2: Ch·ªçn 1 c√¢u hay nh·∫•t + l√Ω do (Pydantic)\n",
    "# ------------------------------------------------\n",
    "class PickResult(BaseModel):\n",
    "    best: str = Field(..., description=\"Slogan hay nh·∫•t, nguy√™n vƒÉn.\")\n",
    "    reason: str = Field(..., description=\"L√Ω do ng·∫Øn g·ªçn v√¨ sao ch·ªçn (<= 30 t·ª´).\")\n",
    "\n",
    "# LLM tr·∫£ th·∫≥ng PickResult (kh√¥ng c·∫ßn JsonOutputParser)\n",
    "llm_picker = llm_pick.with_structured_output(PickResult)\n",
    "\n",
    "p2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"B·∫°n l√† gi√°m kh·∫£o s√°ng t·∫°o c·ªßa Fin68. \"\n",
    "     \"Ti√™u ch√≠: r√µ, ƒë·ªôc ƒë√°o, d·ªÖ nh·ªõ, h·ª£p tone & s·∫£n ph·∫©m.\"),\n",
    "    (\"assistant\", \"Danh s√°ch ƒë·ªÅ c·ª≠:\\n{candidates}\"),\n",
    "    (\"human\", \"H√£y ch·ªçn 1 c√¢u hay nh·∫•t v√† tr·∫£ v·ªÅ JSON ƒë√∫ng schema.\")\n",
    "])\n",
    "pick_chain = p2 | llm_picker\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (3) STAGE 3: Vi·∫øt caption qu·∫£ng c√°o 1 c√¢u ng·∫Øn (<=120 k√Ω t·ª±)\n",
    "# ------------------------------------------------\n",
    "p3 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"B·∫°n l√† copywriter c·ªßa Fin68. Vi·∫øt caption qu·∫£ng c√°o 1 c√¢u duy nh·∫•t, \"\n",
    "     \"ng·∫Øn g·ªçn, r√µ r√†ng, h·∫•p d·∫´n, t·ªëi ƒëa 120 k√Ω t·ª±, c√≥ th·ªÉ d√πng emoji nh·∫π nh√†ng.\"),\n",
    "    (\"assistant\", \"Slogan ƒë√£ ch·ªçn: {best}\"),\n",
    "    (\"human\",\n",
    "     \"Vi·∫øt caption cho s·∫£n ph·∫©m {product} v·ªõi tone {tone}. \"\n",
    "     \"Ch·ªâ tr·∫£ l·∫°i 1 c√¢u, kh√¥ng th√™m g√¨ kh√°c.\")\n",
    "])\n",
    "cap_chain = p3 | llm_cap | StrOutputParser()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# DRIVER\n",
    "# ------------------------------------------------\n",
    "def run_campaign(product: str, tone: str):\n",
    "    # (1) Sinh danh s√°ch kh·∫©u hi·ªáu\n",
    "    candidates = gen_chain.invoke({\"product\": product, \"tone\": tone})\n",
    "\n",
    "    # (2) Ch·ªçn kh·∫©u hi·ªáu hay nh·∫•t (PickResult)\n",
    "    picked: PickResult = pick_chain.invoke({\"candidates\": candidates})\n",
    "\n",
    "    # (3) Vi·∫øt caption d·ª±a tr√™n kh·∫©u hi·ªáu t·ªët nh·∫•t\n",
    "    caption = cap_chain.invoke({\n",
    "        \"best\": picked.best,\n",
    "        \"product\": product,\n",
    "        \"tone\": tone\n",
    "    })\n",
    "\n",
    "    return candidates, picked, caption\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    product = \"C√† ph√™ h√≤a tan 3in1\"\n",
    "    tone = \"nƒÉng ƒë·ªông\"\n",
    "\n",
    "    candidates, picked, caption = run_campaign(product, tone)\n",
    "\n",
    "    print(\"=== (1) C√ÅC SLOGAN ƒê·ªÄ C·ª¨ ===\")\n",
    "    print(candidates.strip())\n",
    "\n",
    "    print(\"\\n=== (2) SLOGAN T·ªêT NH·∫§T (JSON) ===\")\n",
    "    print(picked.model_dump_json(indent=2, ensure_ascii=False))\n",
    "\n",
    "    print(\"\\n=== (3) CAPTION 1 C√ÇU ===\")\n",
    "    print(caption.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f90cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> INPUT:\n",
      "{'product': 'C√† ph√™ h√≤a tan 3in1', 'tone': 'nƒÉng ƒë·ªông'}\n",
      "\n",
      "--- (1) C√ÅC SLOGAN ---\n",
      "1. \"Kh∆°i ngu·ªìn nƒÉng l∆∞·ª£ng, b·∫Øt ƒë·∫ßu ng√†y m·ªõi!\"\n",
      "2. \"C√† ph√™ 3in1 ‚Äì Gi·∫£i ph√°p ti·ªán l·ª£i cho cu·ªôc s·ªëng b·∫≠n r·ªôn!\"\n",
      "3. \"M·ªôt ly, ba h∆∞∆°ng v·ªã ‚Äì NƒÉng ƒë·ªông t·ª´ng gi√¢y!\"\n",
      "4. \"Th·ª©c d·∫≠y c√πng 3in1, t·∫≠n h∆∞·ªüng tr·ªçn v·∫πn!\"\n",
      "5. \"C√† ph√™ nhanh ch√≥ng, phong c√°ch s·ªëng hi·ªán ƒë·∫°i!\"\n",
      "\n",
      "--- (2) PICKED (Pydantic) ---\n",
      "{\n",
      "  \"best\": \"C√† ph√™ 3in1 ‚Äì Gi·∫£i ph√°p ti·ªán l·ª£i cho cu·ªôc s·ªëng b·∫≠n r·ªôn!\",\n",
      "  \"reason\": \"C√¢u slogan ng·∫Øn g·ªçn, d·ªÖ nh·ªõ v√† th·ªÉ hi·ªán r√µ t√≠nh ti·ªán l·ª£i c·ªßa s·∫£n ph·∫©m, ph√π h·ª£p v·ªõi ƒë·ªëi t∆∞·ª£ng kh√°ch h√†ng b·∫≠n r·ªôn.\"\n",
      "}\n",
      "\n",
      "--- (3) CAPTION ---\n",
      "Kh·ªüi ƒë·ªông ng√†y m·ªõi v·ªõi c√† ph√™ 3in1 - NƒÉng l∆∞·ª£ng d·ªìi d√†o ch·ªâ sau m·ªôt t√°ch! ‚òïüí•\n",
      "\n",
      ">>> CAPTION ONLY:\n",
      "Kh·ªüi ƒë·∫ßu ng√†y m·ªõi ƒë·∫ßy nƒÉng l∆∞·ª£ng v·ªõi c√† ph√™ h√≤a tan 3in1 ‚Äì ch·ªâ c·∫ßn pha, khu·∫•y v√† t·∫≠n h∆∞·ªüng! ‚òïüí™‚ú®\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Models\n",
    "llm_gen = ChatOpenAI(model=\"gpt-4o-mini-mini\", temperature=0.7)\n",
    "llm_pick = ChatOpenAI(model=\"gpt-4o-mini-mini\", temperature=0.0)\n",
    "llm_cap  = ChatOpenAI(model=\"gpt-4o-mini-mini\", temperature=0.7)\n",
    "\n",
    "# ===== (1) Sinh 5 slogan =====\n",
    "p1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"B·∫°n l√† chuy√™n gia marketing Fin68, vƒÉn phong ng·∫Øn g·ªçn, hi·ªán ƒë·∫°i.\"),\n",
    "    (\"human\",  \"Vi·∫øt 5 slogan cho s·∫£n ph·∫©m {product}, tone {tone}. Ch·ªâ li·ªát k√™.\")\n",
    "])\n",
    "gen_chain = p1 | llm_gen | StrOutputParser()\n",
    "\n",
    "# ===== (2) Ch·ªçn 1 c√¢u hay nh·∫•t (Pydantic) =====\n",
    "class PickResult(BaseModel):\n",
    "    best: str = Field(..., description=\"Slogan hay nh·∫•t\")\n",
    "    reason: str = Field(..., description=\"L√Ω do ng·∫Øn g·ªçn\")\n",
    "\n",
    "llm_picker = llm_pick.with_structured_output(PickResult)\n",
    "\n",
    "p2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"B·∫°n l√† gi√°m kh·∫£o s√°ng t·∫°o Fin68, ƒë√°nh gi√° r√µ‚Äìƒë·ªôc ƒë√°o‚Äìd·ªÖ nh·ªõ.\"),\n",
    "    (\"assistant\", \"Danh s√°ch ƒë·ªÅ c·ª≠:\\n{candidates}\"),\n",
    "    (\"human\", \"Ch·ªçn 1 c√¢u hay nh·∫•t v√† tr·∫£ v·ªÅ JSON ƒë√∫ng schema.\")\n",
    "])\n",
    "pick_chain = p2 | llm_picker  # -> PickResult\n",
    "\n",
    "# ===== (3) Vi·∫øt caption <= 120 k√Ω t·ª± =====\n",
    "p3 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"B·∫°n l√† copywriter Fin68: caption 1 c√¢u, <=120 k√Ω t·ª±, c√≥ th·ªÉ emoji.\"),\n",
    "    (\"assistant\", \"Slogan ƒë√£ ch·ªçn: {best}\"),\n",
    "    (\"human\", \"Vi·∫øt caption cho {product}, tone {tone}. Ch·ªâ tr·∫£ l·∫°i 1 c√¢u.\")\n",
    "])\n",
    "cap_chain = p3 | llm_cap | StrOutputParser()\n",
    "\n",
    "# ---- Helpers (bi·∫øn c√°c thao t√°c l·∫•y tr∆∞·ªùng th√†nh Runnable) ----\n",
    "get_product = RunnableLambda(itemgetter(\"product\"))\n",
    "get_tone    = RunnableLambda(itemgetter(\"tone\"))\n",
    "\n",
    "# N·∫øu 'picked' l√† Pydantic (khuy√™n d√πng with_structured_output) ‚Üí l·∫•y .best\n",
    "get_best_from_pydantic = RunnableLambda(lambda x: x[\"picked\"].best)\n",
    "\n",
    "# N·∫øu ƒë√¥i khi b·∫°n d√πng dict thay v√¨ Pydantic, x√†i b·∫£n fallback n√†y:\n",
    "# get_best_fallback = RunnableLambda(lambda x: x[\"picked\"][\"best\"] if isinstance(x[\"picked\"], dict) else x[\"picked\"].best)\n",
    "\n",
    "# ===== ONE-SHOT v·ªõi RunnablePassthrough =====\n",
    "one_shot_all = (\n",
    "    RunnablePassthrough.assign(candidates=gen_chain)\n",
    "    .assign(picked=pick_chain)\n",
    "    .assign(\n",
    "        caption=(\n",
    "            {\n",
    "                \"best\": get_best_from_pydantic,  # <-- b·ªçc trong RunnableLambda\n",
    "                \"product\": get_product,\n",
    "                \"tone\": get_tone,\n",
    "            } | cap_chain\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "one_shot_caption = (\n",
    "    RunnablePassthrough.assign(candidates=gen_chain)\n",
    "    .assign(picked=pick_chain)\n",
    "    | {\n",
    "        \"best\": get_best_from_pydantic,\n",
    "        \"product\": get_product,\n",
    "        \"tone\": get_tone,\n",
    "    }\n",
    "    | cap_chain\n",
    ")\n",
    "\n",
    "# ---- Demo\n",
    "if __name__ == \"__main__\":\n",
    "    payload = {\"product\": \"C√† ph√™ h√≤a tan 3in1\", \"tone\": \"nƒÉng ƒë·ªông\"}\n",
    "    full = one_shot_all.invoke(payload)\n",
    "\n",
    "    print(\">>> INPUT:\")\n",
    "    print(payload)\n",
    "\n",
    "    print(\"\\n--- (1) C√ÅC SLOGAN ---\")\n",
    "    print(full[\"candidates\"].strip())\n",
    "\n",
    "    print(\"\\n--- (2) PICKED (Pydantic) ---\")\n",
    "    print(full[\"picked\"].model_dump_json(indent=2, ensure_ascii=False))\n",
    "\n",
    "    print(\"\\n--- (3) CAPTION ---\")\n",
    "    print(full[\"caption\"].strip())\n",
    "\n",
    "    print(\"\\n>>> CAPTION ONLY:\")\n",
    "    print(one_shot_caption.invoke(payload).strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8aeb3",
   "metadata": {},
   "source": [
    "\n",
    "### B√†i t·∫≠p nh·ªè\n",
    "- Bi·∫øn chain tr√™n th√†nh **h√†m** nh·∫≠n `product, tone` ‚Üí tr·∫£ v·ªÅ `{best_line, reason}` ·ªü d·∫°ng dict.\n",
    "- Th√™m b∆∞·ªõc 3: vi·∫øt **bi·∫øn th·ªÉ** c·ªßa slogan t·ªët nh·∫•t (3 bi·∫øn th·ªÉ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697a40c",
   "metadata": {},
   "source": [
    "\n",
    "## Bu·ªïi 3 ‚Äì OutputParser: √©p LLM tr·∫£ k·∫øt qu·∫£ c√≥ c·∫•u tr√∫c\n",
    "\n",
    "**B·∫°n s·∫Ω h·ªçc**: `StrOutputParser`, `JsonOutputParser`, `PydanticOutputParser` v√† c√°ch **validate** k·∫øt qu·∫£.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77192246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"title\": \"Nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa LLM\",\n",
      "    \"bullets\": [\n",
      "        \"M√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (Large Language Models - LLM) l√† c√°c m√¥ h√¨nh tr√≠ tu·ªá nh√¢n t·∫°o ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω v√† hi·ªÉu ng√¥n ng·ªØ t·ª± nhi√™n.\",\n",
      "        \"LLM s·ª≠ d·ª•ng m·∫°ng n∆°-ron s√¢u, c·ª• th·ªÉ l√† c√°c ki·∫øn tr√∫c Transformer, ƒë·ªÉ h·ªçc t·ª´ l∆∞·ª£ng d·ªØ li·ªáu vƒÉn b·∫£n kh·ªïng l·ªì.\",\n",
      "        \"C√°c m√¥ h√¨nh n√†y h·ªçc c√°ch d·ª± ƒëo√°n t·ª´ ti·∫øp theo trong m·ªôt c√¢u, gi√∫p ch√∫ng c√≥ kh·∫£ nƒÉng sinh vƒÉn b·∫£n t·ª± ƒë·ªông, tr·∫£ l·ªùi c√¢u h·ªèi v√† th·ª±c hi·ªán c√°c t√°c v·ª• ng√¥n ng·ªØ kh√°c.\",\n",
      "        \"Qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa LLM bao g·ªìm hai giai ƒëo·∫°n ch√≠nh: ti·ªÅn hu·∫•n luy·ªán v√† tinh ch·ªânh. Trong ƒë√≥, ti·ªÅn hu·∫•n luy·ªán gi√∫p m√¥ h√¨nh h·ªçc hi·ªÉu bi·∫øt chung v·ªÅ ng√¥n ng·ªØ, c√≤n tinh ch·ªânh ƒëi·ªÅu ch·ªânh m√¥ h√¨nh cho c√°c t√°c v·ª• c·ª• th·ªÉ.\",\n",
      "        \"Nh·ªù kh·∫£ nƒÉng x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v√† sinh vƒÉn b·∫£n linh ho·∫°t, c√°c LLM ng√†y c√†ng ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong nhi·ªÅu lƒ©nh v·ª±c nh∆∞ d·ªãch m√°y, t∆∞ v·∫•n t·ª± ƒë·ªông, v√† ph√¢n t√≠ch ng·ªØ nghƒ©a.\"\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üß± JSON Output (khuy·∫øn ngh·ªã m√¥ h√¨nh tr·∫£ JSON)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-mini\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tr·∫£ l·ªùi ·ªü d·∫°ng JSON v·ªõi kh√≥a: title, bullets (list).\\n\"\n",
    "    \"Vi·∫øt n·ªôi dung gi·ªõi thi·ªáu ng·∫Øn cho ch·ªß ƒë·ªÅ: {topic}.\"\n",
    ")\n",
    "\n",
    "raw = (prompt | llm | StrOutputParser()).invoke({\"topic\": \"Nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa LLM\"})\n",
    "print(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Gi·ªõi thi·ªáu v·ªÅ LangChain' bullets=['LangChain l√† m·ªôt framework m√£ ngu·ªìn m·ªü.', 'H·ªó tr·ª£ ph√°t tri·ªÉn c√°c ·ª©ng d·ª•ng s·ª≠ d·ª•ng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs).', 'Cung c·∫•p c√°c c√¥ng c·ª• ƒë·ªÉ qu·∫£n l√Ω chu·ªói h·ªôi tho·∫°i v√† t√≠ch h·ª£p v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu kh√°c nhau.', 'H·ªó tr·ª£ c√°c t√≠nh nƒÉng nh∆∞ qu·∫£n l√Ω tr·∫°ng th√°i, b·ªô nh·ªõ v√† t√≠ch h·ª£p API.', 'Th√≠ch h·ª£p cho vi·ªác x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng chatbot, tr·ª£ l√Ω ·∫£o v√† c√°c h·ªá th·ªëng t·ª± ƒë·ªông h√≥a ng√¥n ng·ªØ.']\n",
      "<class '__main__.Intro'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ PydanticOutputParser ‚Äì √©p schema ch·∫∑t ch·∫Ω\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Intro(BaseModel):\n",
    "    title: str = Field(..., description=\"Ti√™u ƒë·ªÅ ng·∫Øn g·ªçn\")\n",
    "    bullets: list[str] = Field(..., description=\"C√°c √Ω ch√≠nh\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Intro)\n",
    "\n",
    "format_instructions = parser.get_format_instructions()\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"H√£y vi·∫øt ph·∫ßn gi·ªõi thi·ªáu v·ªÅ {topic}.\\n{format_instructions}\"\n",
    ").partial(format_instructions=format_instructions)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "out = (prompt | llm | parser).invoke({\"topic\":\"LangChain c∆° b·∫£n\"})\n",
    "print(out)\n",
    "print(type(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "321411f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"title\": {\"description\": \"Ti√™u ƒë·ªÅ ng·∫Øn g·ªçn\", \"title\": \"Title\", \"type\": \"string\"}, \"bullets\": {\"description\": \"C√°c √Ω ch√≠nh\", \"items\": {\"type\": \"string\"}, \"title\": \"Bullets\", \"type\": \"array\"}}, \"required\": [\"title\", \"bullets\"]}\\n```'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fd0d1",
   "metadata": {},
   "source": [
    "\n",
    "### B√†i t·∫≠p nh·ªè\n",
    "- Th√™m tr∆∞·ªùng `reading_time_min: int` v√†o schema v√† y√™u c·∫ßu LLM ∆∞·ªõc l∆∞·ª£ng th·ªùi gian ƒë·ªçc.  \n",
    "- B·∫Øt l·ªói khi LLM tr·∫£ sai ƒë·ªãnh d·∫°ng v√† **retry** v·ªõi h∆∞·ªõng d·∫´n b·ªï sung.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aaf640",
   "metadata": {},
   "source": [
    "\n",
    "## Bu·ªïi 4 ‚Äì Runnable, song song & streaming\n",
    "\n",
    "**B·∫°n s·∫Ω h·ªçc**: `RunnableLambda`, `RunnableParallel`, stream k·∫øt qu·∫£ LCEL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß† RunnableLambda ‚Äì chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu trong pipeline\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Vi·∫øt 3 g·∫°ch ƒë·∫ßu d√≤ng s√∫c t√≠ch v·ªÅ: {topic}\"\n",
    ")\n",
    "\n",
    "def uppercase(x: str) -> str:\n",
    "    return x.upper()\n",
    "\n",
    "chain = prompt | llm | StrOutputParser() | RunnableLambda(uppercase)\n",
    "print(chain.invoke({\"topic\": \"Quy tr√¨nh h·ªçc LangChain\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ö° RunnableParallel ‚Äì ch·∫°y nhi·ªÅu prompt c√πng l√∫c v√† gom k·∫øt qu·∫£\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "p1 = ChatPromptTemplate.from_template(\"ƒê∆∞a 1 ph√©p ·∫©n d·ª• v·ªÅ {topic}.\")\n",
    "p2 = ChatPromptTemplate.from_template(\"ƒê∆∞a 1 v√≠ d·ª• th·ª±c t·∫ø v·ªÅ {topic}.\")\n",
    "p3 = ChatPromptTemplate.from_template(\"C·∫£nh b√°o 1 sai l·∫ßm th∆∞·ªùng g·∫∑p khi h·ªçc {topic}.\")\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    metaphor=(p1 | llm | StrOutputParser()),\n",
    "    example=(p2 | llm | StrOutputParser()),\n",
    "    pitfall=(p3 | llm | StrOutputParser())\n",
    ")\n",
    "\n",
    "result = parallel.invoke({\"topic\":\"LangChain\"})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üü£ Streaming qua LCEL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Vi·∫øt ƒëo·∫°n gi·ªõi thi·ªáu 2 c√¢u v·ªÅ {topic}.\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "for token in chain.stream({\"topic\": \"Output Parser trong LangChain\"}):\n",
    "    print(token, end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a66933",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## √în t·∫≠p nhanh & G·ª£i √Ω luy·ªán t·∫≠p\n",
    "- **Lu·ªìng chu·∫©n**: Prompt ‚Üí LLM ‚Üí Parser ‚Üí Chain (LCEL `|`).  \n",
    "- **Parser**: √©p ƒë·ªãnh d·∫°ng **JSON/Pydantic** ƒë·ªÉ m√°y ƒë·ªçc ƒë∆∞·ª£c.  \n",
    "- **Runnable**: linh ho·∫°t x·ª≠ l√Ω d·ªØ li·ªáu, ch·∫°y song song, stream.\n",
    "\n",
    "### B√†i t·∫≠p cu·ªëi tu·∫ßn\n",
    "1) T·∫°o **pipeline 3 b∆∞·ªõc**: sinh d√†n √Ω ‚Üí vi·∫øt ƒëo·∫°n ‚Üí r√∫t g·ªçn v·ªÅ 120 k√Ω t·ª±.  \n",
    "2) Vi·∫øt **parser Pydantic** cho k·∫øt qu·∫£ g·ªìm `{title, summary, tags[]}`.  \n",
    "3) √Åp d·ª•ng **streaming** cho b∆∞·ªõc r√∫t g·ªçn ƒë·ªÉ hi·ªán ‚Äúƒëang g√µ‚Äù.\n",
    "\n",
    "> Tu·∫ßn sau: **Agent, Tool & Memory** r·ªìi ti·∫øn d·∫ßn sang **LangGraph & Multi-Agent**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
